{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this notebook in Google Colab by clicking here: [Google Colab](https://colab.research.google.com/github/AaronDJohnson/12p5yr_stochastic_analysis/blob/master/tutorials/model_selection.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run these cells if using Colab. Otherwise, skip them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will reset the kernel.\n",
    "# Run this cell, wait until it's done, then run the next.\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install_mambaforge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!mamba install -y -c conda-forge enterprise_extensions la_forge\n",
    "!git clone https://github.com/AaronDJohnson/12p5yr_stochastic_analysis\n",
    "import sys\n",
    "sys.path.insert(0,'/content/12p5yr_stochastic_analysis/tutorials')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `enterprise` to perform model selection\n",
    "\n",
    "In this notebook you will learn:\n",
    "* How to use `enterprise_extensions` to create models with NANOGrav data,\n",
    "* How to perform model selection on the NANOGrav 15-year data set using `HyperModel`.\n",
    "* How to reproduce some of Figure 2 of the NANOGrav 15-year GWB paper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json, sys, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from enterprise_extensions import models, model_utils, hypermodel\n",
    "\n",
    "from h5pulsar.pulsar import FilePulsar\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    datadir = '/content/12p5yr_stochastic_analysis/tutorials/data'\n",
    "else:\n",
    "    datadir = './data'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the full set of Pulsar objects\n",
    "\n",
    "  * These files have been stored as `HDF5` files to make them much faster to load (and take up little space)\n",
    "  \n",
    "  * See the `explore_data.ipynb` tutorial to see what exists in these files and how to load `.par` and `.tim` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 67 pulsars from hdf5 files\n"
     ]
    }
   ],
   "source": [
    "psrs = []\n",
    "for hdf5_file in glob.glob(datadir + '/hdf5/*.hdf5'):\n",
    "    psrs.append(FilePulsar(hdf5_file))\n",
    "print('Loaded {0} pulsars from hdf5 files'.format(len(psrs)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in white noise dictionaries\n",
    "  * We can read-in some previously computed noise properties from single-pulsar white noise analyses. These are things like `EFAC`, `EQUAD`, and (for `NANOGrav`) `ECORR`. \n",
    "\n",
    "  * In practice, we set these white-noise properties as fixed in the low-frequency noise / GW searches to reduce the computational cost of the analysis significantly.\n",
    "\n",
    "  * The noise properties have been stored as `json` files, and are read into a big parameter dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get parameter noise dictionary\n",
    "noise_ng15 = datadir + '/15yr_wn_dict.json'\n",
    "\n",
    "wn_params = {}\n",
    "with open(noise_ng15, 'r') as fp:\n",
    "    wn_params.update(json.load(fp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection: `model_2a` vs. `model_3a`\n",
    "\n",
    "* This notebook reproduces one of the Bayes factors found in figure 2 of the 15-year GWB analysis paper\n",
    "\n",
    "* We want to be able to compute the Bayes factor for a signal in the data. This can be done using the `HyperModel` class, where we choose between a `model_2a` with a common (but uncorrelated) red process in the pulsars, and `model_3a` with a common, HD correlated red process among all pulsars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodels = 2\n",
    "mod_index = np.arange(nmodels)\n",
    "\n",
    "# Make dictionary of PTAs.\n",
    "pta = dict.fromkeys(mod_index)\n",
    "pta[0] = models.model_2a(psrs, noisedict=wn_params, n_gwbfreqs=14,\n",
    "                         tm_marg=True, tm_svd=True)\n",
    "pta[1] = models.model_3a(psrs, noisedict=wn_params, n_gwbfreqs=14,\n",
    "                         tm_marg=True, tm_svd=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In setting up the `HyperModel` in the next cell, we set weights to make the models sample more evenly.\n",
    "* `log_weights` is a list with the same length as the models, and each entry is added to the corresponding log-likelihood\n",
    "* We will undo the `log_weights` later in post-processing the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model = hypermodel.HyperModel(pta, log_weights=[np.log(200), 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding red noise prior draws...\n",
      "\n",
      "Adding GWB uniform distribution draws...\n",
      "\n",
      "Adding gw param prior draws...\n",
      "\n",
      "Adding nmodel uniform distribution draws...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    outDir = '/content/15yr_stochastic_analysis/tutorials/chains/ms_2a3a_chains'\n",
    "else:\n",
    "    outDir = './chains/ms_2a3a_chains'\n",
    "sampler = super_model.setup_sampler(resume=True, outdir=outDir, sample_nmodel=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler for N steps\n",
    "N = int(5e6)  # 5e6 is a good number for a real analysis\n",
    "x0 = super_model.initial_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "# sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermodynamic Integration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosettaprise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
